{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01bb6dc1",
   "metadata": {},
   "source": [
    "# Objective-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e1114",
   "metadata": {},
   "source": [
    "1. Implemetaion of Web Scrapping using python\n",
    "2. Sentence tokenization\n",
    "3. How to improvise sentence tokenization\n",
    "4. Word tokenization\n",
    "5. Improvisation of word tokenisation\n",
    "6. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ccee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import bs4\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49697e",
   "metadata": {},
   "source": [
    "### Function for wikipedia scrapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeWikiArticle(url):\n",
    "    response = requests.get(\n",
    "        url=url\n",
    "    )\n",
    "    text=\"\"\n",
    "    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    title = soup.find(id=\"firstHeading\").getText()\n",
    "    content = soup.find_all('p')\n",
    "    text=text+title+\" \"\n",
    "    \n",
    "    for i in content:\n",
    "        text=text+i.getText()\n",
    "    return text\n",
    "\n",
    "text=scrapeWikiArticle(\"https://en.wikipedia.org/wiki/COVID-19\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a18bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21482e45",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341130da",
   "metadata": {},
   "source": [
    "##### 1- Lets directly tokenize sentence and see what happens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97bd2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SENTENCES as tokens\n",
    "sentences =sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of_sentences tokenized :\", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ea076",
   "metadata": {},
   "source": [
    "###### we got total 480 sentences, lets see whats in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b853a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentences:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc1dea",
   "metadata": {},
   "source": [
    "###### In above results u can see , we are getting setences with \"[45], [67] etc\", /n, so we got text but need to remove these unwanted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bce7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets pre process the text and see how many sentences are there  after the correction\n",
    "text_corr = re.sub(r'\\[.*?\\]+', '', text)\n",
    "text_corr = sent_corr.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef8a37",
   "metadata": {},
   "source": [
    "###### Lets see how many sentences got corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sentences =sent_tokenize(text_corr)\n",
    "print(\"No of_sentences tokenized :\", len(corr_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total errors in found in sentence tokenization process\n",
    "print(\"Total errors found: \",len(sentences)-len(corr_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aec040",
   "metadata": {},
   "source": [
    "#### Reasons for error in sentence tokenization -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d06be",
   "metadata": {},
   "source": [
    "1. There were boxes like structure i.e [23],[25] etc\n",
    "2. There were multiple /n\n",
    "3. Sentence tokenizer was creating new sentence on every dot. So, if  it is Mr. Raman . it will break into two sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d952929",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd7848",
   "metadata": {},
   "source": [
    "###### lets see directly tokenizing the words and see what it prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a92b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(text)\n",
    "print(\"No of words tokenized: \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574248ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in words:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc44a6c",
   "metadata": {},
   "source": [
    "######  Now in above word tokenization, we could see, it is considering \".\",\"5\",\"]\",\"[\", everything a single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd74cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this step will remove the unwanted brackets as /n as words\n",
    "corr_words=word_tokenize(text_corr)\n",
    "print(\"No of words tokenized: \", len(corr_words))\n",
    "\n",
    "#this step will remove the \"(\",\")\",\",\",\".\" from the list\n",
    "corr_words=list(filter(lambda a: a not in[\"(\",\")\",\".\",\",\",\"``\",\"''\",\"'\",\":\",\";\"], corr_words))\n",
    "print(\"Final No of words tokenized AFTER all filtering: \", len(corr_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in corr_words:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc6d35",
   "metadata": {},
   "source": [
    "###### Lets see the difference between ---- No of words after word tokenization AND No of words after filtering afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total errors in word tokenization: \", len(words)-len(corr_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec788ed3",
   "metadata": {},
   "source": [
    "### Reasons of error-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487e73a",
   "metadata": {},
   "source": [
    "1. All brackets [ , ] , ( ,) , commas , dots, ; , :  these all the things were treated as words \n",
    "2. /n were consider as words\n",
    "3. if you directly copy the data from wikipedia , [] will be a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8817e6",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab88400",
   "metadata": {},
   "source": [
    "We cannot directly apply tokenization on the web scrapped data. Due to different formattings it will give many more unnecessary outputs, which will lead to lesser efficiency of the project.\n",
    "\n",
    "We need to pre-process the data before applying the tokenization.\n",
    "Using regex we can filter various unnecessary segments from the text and can deliver the best possible text to tokenize, will result into maximum accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
