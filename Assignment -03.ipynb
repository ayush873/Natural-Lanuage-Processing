{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0ecacf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import bs4\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import PyPDF2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c57c1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendData(text):\n",
    "    file = open(\"text_file.txt\", \"a\",encoding='utf-8')\n",
    "    file.write(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "58d960bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeWikiArticle(url):\n",
    "    response = requests.get(\n",
    "        url=url\n",
    "    )\n",
    "    text=\"\"\n",
    "    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    title = soup.find(id=\"firstHeading\").getText()\n",
    "    content = soup.find_all('p')\n",
    "    text=text+title+\" \"\n",
    "    \n",
    "    for i in content:\n",
    "        text=text+i.getText()\n",
    "\n",
    "    appendData(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a6884337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeNews(url):\n",
    "    response = requests.get(\n",
    "        url=url\n",
    "    )\n",
    "    text=\"\"\n",
    "    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    title = soup.find('h1').getText()\n",
    "    content = soup.find_all('p')\n",
    "    text=text+title+\" \"\n",
    "    \n",
    "    for i in content:\n",
    "        text=text+i.getText()\n",
    "\n",
    "    appendData(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c224919",
   "metadata": {},
   "source": [
    "### Appending the wikipedia articles to the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "877ffe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapeWikiArticle(\"https://en.wikipedia.org/wiki/COVID-19\")\n",
    "scrapeWikiArticle(\"https://en.wikipedia.org/wiki/Squid_Game\")\n",
    "scrapeWikiArticle(\"https://en.wikipedia.org/wiki/You_(season_3)\")\n",
    "scrapeWikiArticle(\"https://en.wikipedia.org/wiki/Dune_(2021_film)\")\n",
    "scrapeWikiArticle(\"https://en.wikipedia.org/wiki/No_Time_to_Die\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d74e02f",
   "metadata": {},
   "source": [
    "### Appending News feed-RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ac98aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapeNews(\"https://www.nytimes.com/2021/11/01/world/australia/australia-border-reopening-covid.html\")\n",
    "scrapeNews(\"https://www.nytimes.com/2021/11/01/world/europe/britain-climate-change.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f4f6a5",
   "metadata": {},
   "source": [
    "###  Fetching data from pdf file and writing it into text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "67db4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfFileObj = open('ML.pdf', 'rb',)\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "i=31\n",
    "for j in range(12):\n",
    "    pageObj = pdfReader.getPage(i+j)\n",
    "    appendData(pageObj.extractText()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5e73a",
   "metadata": {},
   "source": [
    "### Now, our text file is ready, lets read the data from the text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8141f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalFile = open(\"assignment3.txt\",\"r+\",encoding='utf-8') \n",
    "text=finalFile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb58ebc",
   "metadata": {},
   "source": [
    "## Applying word and sentence tokenization on above text (Assignment 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c8c93",
   "metadata": {},
   "source": [
    "#### 1. Directly tokenizing the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "71b6487c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of_sentences tokenized before applying regex: 2727\n"
     ]
    }
   ],
   "source": [
    "sentences =sent_tokenize(text)\n",
    "print(\"No of_sentences tokenized before applying regex:\", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31fefe5",
   "metadata": {},
   "source": [
    "####  2. Applying regex before tokenizing sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c5f5f3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of_sentences tokenized after applying regex: 2132\n"
     ]
    }
   ],
   "source": [
    "text_corr = re.sub(r'\\[.*?\\]+', '', text)\n",
    "text_corr = text_corr.replace('\\n', '')\n",
    "corr_sentences =sent_tokenize(text_corr)\n",
    "print(\"No of_sentences tokenized after applying regex:\", len(corr_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22c13d",
   "metadata": {},
   "source": [
    "####  3. Finding the sentence we corrcted using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "02576b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(li1, li2):\n",
    "    li_dif = []\n",
    "    for i in li1:\n",
    "        if(i not in li2):\n",
    "            li_dif.append(i)\n",
    "    return li_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00e15963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences with errors that we corrected:  2044\n"
     ]
    }
   ],
   "source": [
    "incorr_sent=Diff(sentences, corr_sentences)\n",
    "print(\"Total sentences with errors that we corrected: \",len(incorr_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40247d25",
   "metadata": {},
   "source": [
    "#### 4. Directly applying word tokenization to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3f10a7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of words tokenized before applying regex  85982\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(text)\n",
    "print(\"No of words tokenized before applying regex \", len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590dcae",
   "metadata": {},
   "source": [
    "#### 5. filtering wrong words and then tokenizing the words\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d378df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of incorrect words tokenized AFTER all filtering:  13315\n"
     ]
    }
   ],
   "source": [
    "incorr_words=[]\n",
    "for i in words:\n",
    "    if(len(i)==1):\n",
    "        res=re.sub(r'[^\\w]' , \"\" ,i)\n",
    "        if(res==\"\"):\n",
    "            incorr_words.append(i)\n",
    "    \n",
    "print(\"Total number of incorrect words tokenized AFTER all filtering: \", len(incorr_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "655901d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72667\n"
     ]
    }
   ],
   "source": [
    "corr_words=Diff(words,incorr_words)\n",
    "print(len(corr_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886a1d3",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ee449418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of_sentences tokenized before applying regex: 2727\n",
      "No of_sentences tokenized after applying regex: 2132\n",
      "Total sentences with errors that we corrected:  2044\n",
      "No of words tokenized before applying regex  85982\n",
      "Total number of incorrect words tokenized AFTER filtering:  13315\n"
     ]
    }
   ],
   "source": [
    "print(\"No of_sentences tokenized before applying regex:\", len(sentences))\n",
    "print(\"No of_sentences tokenized after applying regex:\", len(corr_sentences))\n",
    "print(\"Total sentences with errors that we corrected: \",len(incorr_sent))\n",
    "print(\"No of words tokenized before applying regex \", len(words))\n",
    "print(\"Total number of incorrect words tokenized AFTER filtering: \", len(incorr_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
